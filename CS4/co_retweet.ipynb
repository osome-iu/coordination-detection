{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4: Co-retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amplification of information sources is perhaps the most common form of manipulation. \n",
    "On Twitter, a group of accounts retweeting the same tweets or the same set of accounts may signal coordinated behavior. Therefore we focus on retweets in this case study.\n",
    "\n",
    "We apply the proposed approach to detect coordinated accounts that amplify narratives related to the White Helmets, a volunteer organization that was [targeted by disinformation campaigns during the civil war in Syria](www.theguardian.com/world/2017/dec/18/syria-white-helmets-conspiracy-theories). Recent reports identify Russian sources behind these campaigns. The data was collected from Twitter using English and Arabic keywords. \n",
    "\n",
    "## Detection\n",
    "\n",
    "We construct the bipartite network between retweeting accounts and retweeted messages, excluding self-retweets and accounts having less than ten retweets.\n",
    "This network is weighted using TF-IDF to discount the contributions of popular tweets.\n",
    "Each account is therefore represented as a TF-IDF vector of retweeted tweet IDs.\n",
    "The projected co-retweet network is then weighted by the cosine similarity between the account vectors.\n",
    "Finally, to focus on evidence of potential coordination, we keep only the most suspicious 0.5\\% of the edges.  These parameters can be tuned to trade off between precision and recall; the effect of the thresholds on the precision is analyzed in the Discussion section of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Parameter          | Description                  |\n",
    "|--------------------|------------------------------|\n",
    "| Conjecture         | High overlapping of retweets |\n",
    "| Support filter     | Accounts with <10 retweets   |\n",
    "| Trace              | Retweeted tweet ID           |\n",
    "| Eng. trace         | No                           |\n",
    "| Bipartite weight   | TF-IDF                       |\n",
    "| Proj. weight       | Cosine similarity            |\n",
    "| Edge filter        | Keep top 0.5% weights        |\n",
    "| Clustering         | Connected components         |\n",
    "| Data source        | DARPA SocialSim              |\n",
    "| Data period        | Apr 2018–Mar 2019            |\n",
    "| No. accounts       | 11,669                       |\n",
    "\n",
    "*Table: Case study 3 summary*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from itertools import combinations\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "def counters2edges(counter_dict, p1_col, p2_col, w_col):\n",
    "    # Count how many times each feature co-occurs with each user\n",
    "    edge = pd.DataFrame.from_records(\n",
    "        [\n",
    "            (uid, feature, count)\n",
    "            for uid, feature_counter in counter_dict.items()\n",
    "            for feature, count in feature_counter.items()\n",
    "        ],\n",
    "        columns=[p1_col, p2_col, w_col],\n",
    "    )\n",
    "    return edge\n",
    "\n",
    "\n",
    "def prepare_content(in_fp, tqdm_desc=\"parse raw content\", tqdm_total=None):\n",
    "    \"\"\"\n",
    "    Parse retweet ids from a raw tweet file.\n",
    "    Note that the fields in here are different from the usual Twitter fields, because this is a anonymized dataset\n",
    "    i.e., we access the attribute \"id_h\" instead of \"id_str\"\n",
    "    \"\"\"\n",
    "    user_features = defaultdict(lambda: defaultdict(Counter))\n",
    "    idx = 0\n",
    "    for line in tqdm(in_fp, desc=tqdm_desc, total=tqdm_total):\n",
    "        twt = json.loads(line)\n",
    "        if \"user\" not in twt:\n",
    "            continue\n",
    "\n",
    "        user_id = twt[\"user\"][\"id_h\"]\n",
    "        if \"retweeted_status\" in twt:\n",
    "            try:\n",
    "                rtwt = twt[\"retweeted_status\"]\n",
    "                retweeted_tweet_id = rtwt[\"id_h\"]\n",
    "                user_features[\"retweet_tid\"][user_id][retweeted_tweet_id] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"pass {idx}\")\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "    return user_features\n",
    "\n",
    "\n",
    "def dummy_func(doc):\n",
    "    return doc\n",
    "\n",
    "\n",
    "def calculate_interaction(\n",
    "    edge_df,\n",
    "    p1_col,\n",
    "    p2_col,\n",
    "    w_col,\n",
    "    node1_col,\n",
    "    node2_col,\n",
    "    sim_col,\n",
    "    sup_col,\n",
    "    tqdm_total=None,\n",
    "    tqdm_desc=\"calculating interaction\",\n",
    "):\n",
    "    \"\"\"\n",
    "    calculate interactions between nodes in partition 1\n",
    "\n",
    "    input:\n",
    "        edge_df : dataframe that contains (p1, p2, w)\n",
    "        p1_col : column name that represent p1, i.e: uid\n",
    "        p2_col : column name that represent p2 , e.g: time\n",
    "        w_col : column name that represent weight, if None it's unweighted\n",
    "        node1_col, node2_col: col after projection, both are uid\n",
    "        supports: dictionary of userids - post count {p1_node: support}\n",
    "    return:\n",
    "        interactions : dict { (p1_node1, p1_node2) : interaction_weight }\n",
    "\n",
    "    assumption:\n",
    "        1. non-empty inputs: edge_df\n",
    "        2. p1_node1 < p1_node2 holds for the tuples in interactions\n",
    "    \"\"\"\n",
    "    user_ids, docs = zip(\n",
    "        *(\n",
    "            (p1_node, grp[p2_col].repeat(grp[w_col]).values)\n",
    "            for p1_node, grp in edge_df.groupby(p1_col)\n",
    "            if len(grp) > 1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        tfidf = TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            tokenizer=dummy_func,\n",
    "            preprocessor=dummy_func,\n",
    "            use_idf=True,\n",
    "            token_pattern=None,\n",
    "            lowercase=False,\n",
    "            sublinear_tf=True,\n",
    "            min_df=3,\n",
    "        )\n",
    "        docs_vec = tfidf.fit_transform(docs)\n",
    "    except ValueError:\n",
    "        tfidf = TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            tokenizer=dummy_func,\n",
    "            preprocessor=dummy_func,\n",
    "            use_idf=True,\n",
    "            token_pattern=None,\n",
    "            lowercase=False,\n",
    "            sublinear_tf=True,\n",
    "            min_df=1,\n",
    "        )\n",
    "        docs_vec = tfidf.fit_transform(docs)\n",
    "\n",
    "    results = sp_matmul_topn(docs_vec, docs_vec.T, top_n=100)\n",
    "\n",
    "    interactions = pd.DataFrame.from_records(\n",
    "        [\n",
    "            (\n",
    "                u1,\n",
    "                user_ids[u2_idx],\n",
    "                sim,\n",
    "                min(supports[user_ids[u1_idx]], supports[user_ids[u2_idx]]),\n",
    "            )\n",
    "            for u1_idx, u1 in enumerate(user_ids)\n",
    "            for u2_idx, sim in zip(\n",
    "                results.indices[results.indptr[u1_idx] : results.indptr[u1_idx + 1]],\n",
    "                results.data[results.indptr[u1_idx] : results.indptr[u1_idx + 1]],\n",
    "            )\n",
    "            if u1_idx < u2_idx\n",
    "        ],\n",
    "        columns=[node1_col, node2_col, sim_col, sup_col],\n",
    "    )\n",
    "\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of lines to use progress bar (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets\n",
    "raw_json_gz = f\"data/sample_raw_tweets.json.gz\"\n",
    "# number of lines in the input file\n",
    "numline_file = f\"data/sample_raw_tweets.numline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {raw_json_gz} | wc -l > {numline_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1850502"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numline = int(open(numline_file).read().strip())\n",
    "numline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Parse json of tweets to a table of user - feature values \n",
    "\n",
    "Construct a uid - feature - count table from raw tweets \n",
    "\n",
    "Input: raw tweet.json.gz files \n",
    "\n",
    "Output: feature parquets\n",
    "In this case study, the feature are the tweet IDs that a user retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parse raw content:  32%|███▏      | 600000/1850502 [00:37<01:19, 15801.26it/s]\n"
     ]
    }
   ],
   "source": [
    "outdir = \"results\"\n",
    "# column name of nodes in partite 1 and 2\n",
    "p1_col = \"uid\"\n",
    "p2_col = \"feature\"\n",
    "# column name of edge weights\n",
    "w_col = \"cnt\"\n",
    "\n",
    "feature = \"retweet_id\"\n",
    "# read input\n",
    "with gzip.open(raw_json_gz, \"rb\") as in_fp:\n",
    "    # do work\n",
    "    user_features = prepare_content(in_fp, tqdm_total=numline)\n",
    "\n",
    "# write output\n",
    "for feature, counter_dict in user_features.items():\n",
    "    edge_df = counters2edges(counter_dict, p1_col, p2_col, w_col)\n",
    "\n",
    "    fname = \"{}.edge.parquet\".format(feature)\n",
    "    edge_df.to_parquet(join(outdir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the feature dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>feature</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GpiuNvbRtbXwfynWiD_czA</td>\n",
       "      <td>XQFFKrN4jekP1eBSssFQsQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N1t6Jzysm3Y885HGvTXLkQ</td>\n",
       "      <td>XQFFKrN4jekP1eBSssFQsQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nLUp9Cli7fPuBSO8ghqgtQ</td>\n",
       "      <td>Clnzx7_oryrhThV-XRBSmQ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nLUp9Cli7fPuBSO8ghqgtQ</td>\n",
       "      <td>DvoA5c_dijc9wPLa06t50Q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FulMsCaamosiAYela5pgrg</td>\n",
       "      <td>t3YbgFjNzocBgkeeW3khyA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      uid                 feature  cnt\n",
       "0  GpiuNvbRtbXwfynWiD_czA  XQFFKrN4jekP1eBSssFQsQ    1\n",
       "1  N1t6Jzysm3Y885HGvTXLkQ  XQFFKrN4jekP1eBSssFQsQ    1\n",
       "2  nLUp9Cli7fPuBSO8ghqgtQ  Clnzx7_oryrhThV-XRBSmQ    1\n",
       "3  nLUp9Cli7fPuBSO8ghqgtQ  DvoA5c_dijc9wPLa06t50Q    1\n",
       "4  FulMsCaamosiAYela5pgrg  t3YbgFjNzocBgkeeW3khyA    1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df_name = f\"{outdir}/{feature}.edge.parquet\"\n",
    "pd.read_parquet(edge_df_name).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Calculate the similarity between features \n",
    "\n",
    "We consider each user as a document, and \"words\" are the feature scores, i.e., retweet ids. \n",
    "- We first filter out users with lack of support, i.e., those reposts less than `sup_thresh = 10` times\n",
    "\n",
    "- We then create a TF-IDF vector for each user, where each dimension is a retweet id, and the value is the number of retweets.\n",
    "\n",
    "We output a dataframe with the following columns: `user1, user2, similarity, support`; where each record represents an interaction between `user1` and `user2`; `similarity` is the strength of connections and `support` is the post count of `user1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No users:  154414\n",
      "No users after filtering with threshold=8: 4095\n"
     ]
    }
   ],
   "source": [
    "# column name of node1, node2, similarity, support\n",
    "node1_col = \"user1\"\n",
    "node2_col = \"user2\"\n",
    "sim_col = \"similarity\"\n",
    "sup_col = \"support\"\n",
    "sup_thresh = 8\n",
    "# takes a .parquet file of edge table\n",
    "# save to output file of interactions with p-values\n",
    "interaction_file = f\"{outdir}/{feature}.interactions.parquet\"\n",
    "# read input\n",
    "edge_df = pd.read_parquet(edge_df_name)\n",
    "\n",
    "if not (len(edge_df) > 0 and np.any(edge_df.groupby(p1_col).size() > 1)):\n",
    "    raise ValueError(\n",
    "        \"Empty input edge table. Run `prepare_content()` to create edge table first\"\n",
    "    )\n",
    "else:\n",
    "    # calculate the support, which is the post count for each user\n",
    "    supports = edge_df.groupby(p1_col)[w_col].sum().to_dict()\n",
    "    print(\"No users: \", len(supports))\n",
    "    # filter nodes based on support\n",
    "    supports = {k: v for k, v in supports.items() if v > sup_thresh}\n",
    "    print(f\"No users after filtering with threshold={sup_thresh}: {len(supports)}\")\n",
    "    filtered_nodes = list(supports.keys())\n",
    "    # remove the nodes with low support from edge_df\n",
    "    edge_df = edge_df[edge_df[p1_col].isin(filtered_nodes)]\n",
    "\n",
    "    # calculate similirity between users\n",
    "    interaction_df = calculate_interaction(\n",
    "        edge_df, p1_col, p2_col, w_col, node1_col, node2_col, sim_col, sup_col\n",
    "    )\n",
    "    # write output\n",
    "    interaction_df.to_parquet(interaction_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the interaction df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user1</th>\n",
       "      <th>user2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1Yb7yo_soaiKjc5x3biGQ</td>\n",
       "      <td>myVI0yg25Ws-HHLvOMwtyw</td>\n",
       "      <td>0.264076</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1Yb7yo_soaiKjc5x3biGQ</td>\n",
       "      <td>lnHATpuKdTSVO40BbCJETQ</td>\n",
       "      <td>0.232556</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1Yb7yo_soaiKjc5x3biGQ</td>\n",
       "      <td>a6Wn9Gtt28SxNe6BKnsJIw</td>\n",
       "      <td>0.291757</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1Yb7yo_soaiKjc5x3biGQ</td>\n",
       "      <td>DuwH448XfvmUrM6hDDYvYw</td>\n",
       "      <td>0.285331</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1Yb7yo_soaiKjc5x3biGQ</td>\n",
       "      <td>718SpdW5td0JVg7t4a7daQ</td>\n",
       "      <td>0.294394</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    user1                   user2  similarity  support\n",
       "0  -1Yb7yo_soaiKjc5x3biGQ  myVI0yg25Ws-HHLvOMwtyw    0.264076       11\n",
       "1  -1Yb7yo_soaiKjc5x3biGQ  lnHATpuKdTSVO40BbCJETQ    0.232556       12\n",
       "2  -1Yb7yo_soaiKjc5x3biGQ  a6Wn9Gtt28SxNe6BKnsJIw    0.291757        9\n",
       "3  -1Yb7yo_soaiKjc5x3biGQ  DuwH448XfvmUrM6hDDYvYw    0.285331        9\n",
       "4  -1Yb7yo_soaiKjc5x3biGQ  718SpdW5td0JVg7t4a7daQ    0.294394        9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_df = pd.read_parquet(interaction_file)\n",
    "interaction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Filter low edge weight and create a network \n",
    "For this case study, we apply an weight filter `sim_thresh = 0.995`, i.e., only accounts with similarity >0.995 percentile are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to input indexed text of tweets\n",
    "twttext = f\"{outdir}/table.by.user.pkl\"\n",
    "\n",
    "# path to output file of the graphml for Gephi visualization\n",
    "outgraph = f\"{outdir}/{feature}.filtered.coord.graphml\"\n",
    "# path to output file of context of each suspicious group\n",
    "grouptext = f\"{outdir}/{feature}filtered.coord.pkl\"\n",
    "\n",
    "# read input parquet file of interaction\n",
    "interaction_df = pd.read_parquet(interaction_file)\n",
    "\n",
    "# define the filters\n",
    "sim_thresh = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# apply edge weight filter: 0.995 quantile cosine similarity of TFIDF vectors\n",
    "interaction_df = interaction_df[\n",
    "    interaction_df[sim_col] >= interaction_df[sim_col].quantile(sim_thresh)\n",
    "]\n",
    "# create a graph object from the interaction table\n",
    "G = nx.Graph()\n",
    "for idx, row in interaction_df.iterrows():\n",
    "    try:\n",
    "        G.add_edge(\n",
    "            row[node1_col], row[node2_col], weight=row[sim_col], support=row[sup_col]\n",
    "        )\n",
    "    except:\n",
    "        print(row)\n",
    "        pass\n",
    "#  write graph object to file\n",
    "nx.write_gml(G, outgraph)\n",
    "del interaction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "The above code matches the filtering procedure described in the paper: \n",
    "1. users with low support (few posts) are removed\n",
    "2. tf-idf vectors are calculated\n",
    "3. similarity between tf-idf vectors are calculated, and edges with low similarity are removed. \n",
    "\n",
    "The results of case study 5 in the paper were obtained by a slightly different ordering of these steps:\n",
    "1. tf-idf vectors are calculated\n",
    "2. similarity between tf-idf vectors are calculated, and edges with low similarity are removed\n",
    "3. users with low support are removed. \n",
    "\n",
    "The resulting networks are very similar, and the result reported in the paper are not affected by this change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coordination",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
