{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study 3\n",
        "\n",
        "A  data  preprocessing  step  filters  out  accounts  with  few tweets and hashtags.\n",
        "The thresholds depend on the time period under evaluation. In this case we use a minimum of\n",
        "five tweets and five unique hashtags over a period of 24 hours to ensure sufficient\n",
        "support for possible coordination.\n",
        "\n",
        "We  engineer  features  that  combine  content (hashtags) and activity (timestamps) traces.\n",
        "In particular, we use ordered  sequences of  hashtags  for  each  user. The bipartite\n",
        "network consists of accounts in one layer and hashtag sequences in the other.\n",
        "In the projection phase, we draw an edge between two accounts with identical hashtag sequences.\n",
        "These edges are unweighted and we do not apply any filtering, based on the assumption\n",
        "that two independent users are unlikely to post identical sequences of five or more hashtags\n",
        "on the same day.\n",
        "\n",
        "We identify suspicious groups of accounts by removing singleton nodes and then extracting\n",
        "the connected components of  the  network.  Large  components  are  more  suspicious,\n",
        "as  it  is  less  likely  that  many  accounts  post  the same hashtag sequences by chance.\n",
        "\n",
        "| Conjecture           | Similar large sequence of hashtags                                               |\n",
        "|----------------------|----------------------------------------------------------------------------------|\n",
        "| Support filter       | At least 5 tweets, 5 hashtags per day                                           |\n",
        "| Trace                | Hashtags in a tweet                                                             |\n",
        "| Eng. trace           | Ordered sequence of hashtags in a day                                           |\n",
        "| Bipartite weight     | NA, the bipartite is unweighted                                                 |\n",
        "| Proj. weight         | Co-occurrence                                                                   |\n",
        "| Edge filter          | No                                                                             |\n",
        "| Clustering           | Connected components                                                           |\n",
        "| Data source          | BEV (Yang, Hui, and Menczer 2019)                                              |\n",
        "| Data period          | Octâ€“Dec 2018                                                                   |\n",
        "| No. accounts         | 59,389,305                                                                     |\n",
        "\n",
        "*Table 3: Case study 3 summary*"
      ],
      "metadata": {
        "id": "MzGj_qLqPtKp"
      },
      "id": "MzGj_qLqPtKp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3530934-93b6-42a7-aec5-7732320dced7",
      "metadata": {
        "id": "f3530934-93b6-42a7-aec5-7732320dced7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4166a4a9-9b64-4096-9cbf-745016e42639",
      "metadata": {
        "id": "4166a4a9-9b64-4096-9cbf-745016e42639"
      },
      "source": [
        "### The tweets file should only contain native tweets from the v1.1 Twitter API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20efadc0-b21e-4c1a-b87f-eeddd129f4ce",
      "metadata": {
        "id": "20efadc0-b21e-4c1a-b87f-eeddd129f4ce"
      },
      "outputs": [],
      "source": [
        "# minimum number of tweets per user\n",
        "min_tweets = 5\n",
        "\n",
        "# minimum number of hashtags\n",
        "min_unique_hashtags = 5\n",
        "min_tweets_with_hashtags = 5\n",
        "\n",
        "tweetid = []\n",
        "text = []\n",
        "hashtags = []\n",
        "userid = []\n",
        "created_at = []\n",
        "\n",
        "# The tweets json file is a daily dump of tweets\n",
        "with gzip.open('tweets.json.gz', 'rb') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            tmp_json = json.loads(line)\n",
        "            for row in range(len(tmp_json)):\n",
        "                tweetid.append(tmp_json[row]['id_str'])\n",
        "                text.append(tmp_json[row]['text'])\n",
        "                userid.append(tmp_json[row]['user']['id_str'])\n",
        "                created_at.append(pd.to_datetime(tmp_json[row]['created_at']))\n",
        "                hashtags.append(tmp_json[row]['entities']['hashtags'])\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "data = pd.DataFrame(tweetid, columns=['tweetid'])\n",
        "data['userid'] = userid\n",
        "data['created_at'] = created_at\n",
        "data['text'] = text\n",
        "data['hashtags'] = hashtags\n",
        "data.sort_values(by='created_at', inplace=True)\n",
        "\n",
        "# Identify users that published the same sequence of hashtags\n",
        "hashtag_seqs = dict()\n",
        "# We iterate over each user in the dataframe\n",
        "for user in data['userid'].unique():\n",
        "    hashtag_sequence_list = []\n",
        "    twts_with_hashtags_count = 0\n",
        "    # For users that have at least the minimum required number of tweets,\n",
        "    if len(data[data['userid'] == user].index) >= min_tweets:\n",
        "        # we count how many of them have hashtags\n",
        "        # and keep track of the hashtags\n",
        "        for index in data[data['userid'] == user].index:\n",
        "            if len(data.loc[index,'hashtags']) != 0:\n",
        "                twts_with_hashtags_count += 1\n",
        "                for hashtag in data.loc[index,'hashtags']:\n",
        "                    hashtag_sequence_list.append(hashtag['text'])\n",
        "        # If an account has at least the minimum required number of tweets that contain hashtags\n",
        "        # and enough of them are unique, then we add the user to a dictionary that\n",
        "        # keeps track of qualifying users\n",
        "        if (len(set(hashtag_sequence_list)) >= min_unique_hashtags) & (twts_with_hashtags_count >= min_tweets_with_hashtags):\n",
        "            hashtag_sequence_str = \"\".join(hashtag_sequence_list)\n",
        "            if hashtag_sequence_str in hashtag_seqs.keys():\n",
        "                hashtag_seqs[hashtag_sequence_str].append(user)\n",
        "            else:\n",
        "                hashtag_seqs[hashtag_sequence_str] = [user]\n",
        "\n",
        "# Build an undirect network of users that met the above thresholds and\n",
        "# have the same hashtag sequences. The nodes (users) in the network will be connected\n",
        "# to other users with the same hashtag sequence\n",
        "G = nx.Graph()\n",
        "for key in hashtag_seqs.keys():\n",
        "    if len(hashtag_seqs[key]) > 1:\n",
        "        for comb in list(combinations(hashtag_seqs[key], 2)):\n",
        "            nx.add_edge(comb[0], comb[1])\n",
        "\n",
        "nx.write_gexf(G,'hashtag_coordination_graph.gexf')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}